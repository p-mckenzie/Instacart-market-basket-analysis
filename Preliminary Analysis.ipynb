{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "### Instacart Market Basket Analysis - Paige McKenzie\n",
    "\n",
    "See my [blog post](https://p-mckenzie.github.io/content/python/2017/12/12/instacart-part-1/ \"Instacart Part 2 - Modeling\") on the subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages and read in data, created by feature_engineering.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>target</th>\n",
       "      <th>avg_order_size</th>\n",
       "      <th>prev_ord_size</th>\n",
       "      <th>avg_days_between_orders</th>\n",
       "      <th>num_orders_placed</th>\n",
       "      <th>reordered_usr_avg</th>\n",
       "      <th>overall_avg_prod_disp</th>\n",
       "      <th>...</th>\n",
       "      <th>usr_avg_aisle_disp</th>\n",
       "      <th>usr_avg_dept_disp</th>\n",
       "      <th>usr_avg_prod_disp</th>\n",
       "      <th>prod_due_overall_perc</th>\n",
       "      <th>prod_due_user_perc</th>\n",
       "      <th>aisle_due_overall_perc</th>\n",
       "      <th>aisle_due_user_perc</th>\n",
       "      <th>dept_due_overall_perc</th>\n",
       "      <th>dept_due_user_perc</th>\n",
       "      <th>reorder_custom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1187899</td>\n",
       "      <td>196</td>\n",
       "      <td>1</td>\n",
       "      <td>5.9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.56</td>\n",
       "      <td>11</td>\n",
       "      <td>0.695</td>\n",
       "      <td>66.272329</td>\n",
       "      <td>...</td>\n",
       "      <td>19.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.695</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1187899</td>\n",
       "      <td>10258</td>\n",
       "      <td>1</td>\n",
       "      <td>5.9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.56</td>\n",
       "      <td>11</td>\n",
       "      <td>0.695</td>\n",
       "      <td>56.355440</td>\n",
       "      <td>...</td>\n",
       "      <td>19.44</td>\n",
       "      <td>19.00</td>\n",
       "      <td>19.44</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.695</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>431534</td>\n",
       "      <td>10326</td>\n",
       "      <td>0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.56</td>\n",
       "      <td>11</td>\n",
       "      <td>0.695</td>\n",
       "      <td>57.078752</td>\n",
       "      <td>...</td>\n",
       "      <td>87.50</td>\n",
       "      <td>87.50</td>\n",
       "      <td>97.00</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2550362</td>\n",
       "      <td>12427</td>\n",
       "      <td>0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.56</td>\n",
       "      <td>11</td>\n",
       "      <td>0.695</td>\n",
       "      <td>76.984336</td>\n",
       "      <td>...</td>\n",
       "      <td>19.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.695</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1187899</td>\n",
       "      <td>13032</td>\n",
       "      <td>1</td>\n",
       "      <td>5.9</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.56</td>\n",
       "      <td>11</td>\n",
       "      <td>0.695</td>\n",
       "      <td>83.045575</td>\n",
       "      <td>...</td>\n",
       "      <td>58.33</td>\n",
       "      <td>58.33</td>\n",
       "      <td>58.33</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.695</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  order_id  product_id  target  avg_order_size  prev_ord_size  \\\n",
       "0        1   1187899         196       1             5.9            9.0   \n",
       "1        1   1187899       10258       1             5.9            9.0   \n",
       "2        1    431534       10326       0             5.9            9.0   \n",
       "3        1   2550362       12427       0             5.9            9.0   \n",
       "4        1   1187899       13032       1             5.9            9.0   \n",
       "\n",
       "   avg_days_between_orders  num_orders_placed  reordered_usr_avg  \\\n",
       "0                    19.56                 11              0.695   \n",
       "1                    19.56                 11              0.695   \n",
       "2                    19.56                 11              0.695   \n",
       "3                    19.56                 11              0.695   \n",
       "4                    19.56                 11              0.695   \n",
       "\n",
       "   overall_avg_prod_disp       ...        usr_avg_aisle_disp  \\\n",
       "0              66.272329       ...                     19.00   \n",
       "1              56.355440       ...                     19.44   \n",
       "2              57.078752       ...                     87.50   \n",
       "3              76.984336       ...                     19.00   \n",
       "4              83.045575       ...                     58.33   \n",
       "\n",
       "   usr_avg_dept_disp  usr_avg_prod_disp  prod_due_overall_perc  \\\n",
       "0              19.00              19.00                  0.909   \n",
       "1              19.00              19.44                  0.818   \n",
       "2              87.50              97.00                  0.182   \n",
       "3              19.00              19.00                  0.909   \n",
       "4              58.33              58.33                  0.273   \n",
       "\n",
       "   prod_due_user_perc  aisle_due_overall_perc  aisle_due_user_perc  \\\n",
       "0               0.909                   0.695                0.909   \n",
       "1               0.909                   0.695                0.818   \n",
       "2               0.182                   0.695                0.182   \n",
       "3               0.909                   0.695                0.909   \n",
       "4               0.273                   0.695                0.273   \n",
       "\n",
       "   dept_due_overall_perc  dept_due_user_perc  reorder_custom  \n",
       "0                  0.909               0.695               1  \n",
       "1                  0.909               0.695               1  \n",
       "2                  0.182               0.695               0  \n",
       "3                  0.909               0.695               1  \n",
       "4                  0.273               0.695               1  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('x_train.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7645837\n",
       "1     828824\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into train/test based on user_id to simulate Kaggle train/test environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "mylist = list(df['user_id'].unique())\n",
    "\n",
    "random.seed(9001)\n",
    "train_users = [ mylist[i] for i in (random.sample(range(len(mylist)), int(.8*len(mylist)))) ]\n",
    "\n",
    "train = df[df['user_id'].isin(train_users)]\n",
    "test = df[~df['user_id'].isin(train_users)]\n",
    "\n",
    "del df\n",
    "del train_users\n",
    "\n",
    "y_train = np.array(train['target'])\n",
    "y_test = np.array(test['target'])\n",
    "\n",
    "# note this ignores aisle/deparment\n",
    "X_train = np.array(train[list(train.columns[4:])])\n",
    "X_test = np.array(test[list(test.columns[4:])])\n",
    "\n",
    "var_names = test.columns[4:]\n",
    "\n",
    "del train\n",
    "del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.575449448601\n"
     ]
    }
   ],
   "source": [
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_train,y_train)\n",
    "\n",
    "pred = log_model.predict(X_test)\n",
    "\n",
    "print roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1515241,   16156],\n",
       "       [ 139332,   26826]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(log_model, open('log.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 - AUC of 0.571545405359 at 14:16:30\n",
      "\n",
      "20 20 - AUC of 0.564028186493 at 14:22:10\n",
      "\n",
      "30 30 - AUC of 0.56824115195 at 14:29:42\n",
      "\n",
      "40 40 - AUC of 0.573391535951 at 14:40:28\n",
      "\n",
      "30 10 - AUC of 0.567336724571 at 14:45:47\n",
      "\n",
      "10 30 - AUC of 0.571902902222 at 14:51:24\n",
      "\n",
      "40 10 - AUC of 0.571801576075 at 14:57:57\n",
      "\n",
      "10 40 - AUC of 0.579734696004 at 15:04:19\n",
      "\n",
      "20 40 - AUC of 0.571531376062 at 15:12:27\n",
      "\n",
      "40 20 - AUC of 0.575220450564 at 15:20:25\n",
      "\n",
      "40 30 - AUC of 0.578366014653 at 15:28:40\n",
      "\n",
      "30 40 - AUC of 0.577656682285 at 15:37:19\n",
      "\n",
      "50 10 - AUC of 0.574887862237 at 15:44:24\n",
      "\n",
      "10 50 - AUC of 0.574112481921 at 15:50:41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_sizes = [(10,10),(20,20),(30,30),(40,40),(30,10),(10,30),(40,10),(10,40),(20,40),(40,20),(40,30),(30,40),(50,10),(10,50)]\n",
    "\n",
    "for hidden_layer in hidden_layer_sizes:\n",
    "    #define MLP\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=hidden_layer, activation='tanh',\n",
    "                       solver='sgd', learning_rate='adaptive',\n",
    "                      random_state=42, batch_size=500, learning_rate_init=.003,\n",
    "                        momentum=.5, tol=.001, verbose=False,\n",
    "                       early_stopping=True, validation_fraction=.2)\n",
    "\n",
    "    #fit to training data\n",
    "    mlp.fit(X_train, y_train)\n",
    "    #predict on test data\n",
    "    pred = mlp.predict(X_test)\n",
    "\n",
    "    #find AUC on out-of-sample user_ids\n",
    "    print('{} {} - AUC of {} at {}\\n'.format(hidden_layer[0], hidden_layer[1], roc_auc_score(y_test, pred), datetime.datetime.now().strftime('%H:%M:%S')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.0001, batch_size=500, beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(10, 40), learning_rate='adaptive',\n",
       "       learning_rate_init=0.003, max_iter=200, momentum=0.5,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=42, shuffle=True,\n",
       "       solver='sgd', tol=0.001, validation_fraction=0.2, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#re-fit best model\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,40), activation='tanh',\n",
    "                       solver='sgd', learning_rate='adaptive',\n",
    "                      random_state=42, batch_size=500, learning_rate_init=.003,\n",
    "                        momentum=.5, tol=.001, verbose=False,\n",
    "                       early_stopping=True, validation_fraction=.2)\n",
    "\n",
    "#fit to training data\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.579734696004\n"
     ]
    }
   ],
   "source": [
    "pred = mlp.predict(X_test)\n",
    "\n",
    "print roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1513389,   18008],\n",
       "       [ 137707,   28451]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(mlp, open('mlp.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(colsample_bylevel=1, colsample_bytree=0.8,\n",
       "       gamma=0, learning_rate=0.2, max_delta_step=0, max_depth=10,\n",
       "       min_child_weight=1, missing=None, n_estimators=140, nthread=-1,\n",
       "       objective='reg:linear', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#re-fit model with best parameters\n",
    "\n",
    "gbm = xgboost.XGBClassifier(objective='binary:logistic',\n",
    "                            max_depth=10,\n",
    "                            learning_rate=.2,\n",
    "                            n_estimators=140,\n",
    "                            subsample=.8,\n",
    "                            colsample_bytree=.8)\n",
    "\n",
    "gbm.fit(X_train, y_train,\n",
    "                eval_set=[(X_test, y_test)],\n",
    "                eval_metric='auc',\n",
    "                early_stopping_rounds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.588788293785\n"
     ]
    }
   ],
   "source": [
    "pred = gbm.predict(X_test)\n",
    "\n",
    "print roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1513819,   17578],\n",
       "       [ 134745,   31413]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(gbm, open('xgb.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\t40\t20\tAUC 0.578251924066 at 20:56:32\n",
      "0.1\t40\t40\tAUC 0.581280609869 at 20:57:46\n",
      "0.1\t40\t80\tAUC 0.583088438328 at 20:59:11\n",
      "0.1\t140\t20\tAUC 0.587328261938 at 21:02:19\n",
      "0.1\t140\t40\tAUC 0.588155576665 at 21:05:52\n",
      "0.1\t140\t80\tAUC 0.589233750318 at 21:09:47\n",
      "0.1\t240\t20\tAUC 0.587838371038 at 21:13:56\n",
      "0.1\t240\t40\tAUC 0.588321732852 at 21:18:07\n",
      "0.1\t240\t80\tAUC 0.589233750318 at 21:22:06\n",
      "0.1\t340\t20\tAUC 0.587838371038 at 21:26:17\n",
      "0.1\t340\t40\tAUC 0.588321732852 at 21:30:29\n",
      "0.1\t340\t80\tAUC 0.589233750318 at 21:34:27\n",
      "0.2\t40\t20\tAUC 0.586425691269 at 21:35:30\n",
      "0.2\t40\t40\tAUC 0.587534467847 at 21:36:44\n",
      "0.2\t40\t80\tAUC 0.588025843342 at 21:38:07\n",
      "0.2\t140\t20\tAUC 0.58806239631 at 21:40:28\n",
      "0.2\t140\t40\tAUC 0.589345595929 at 21:42:43\n",
      "0.2\t140\t80\tAUC 0.588975871948 at 21:44:31\n",
      "0.2\t240\t20\tAUC 0.58806239631 at 21:46:56\n",
      "0.2\t240\t40\tAUC 0.589345595929 at 21:49:16\n",
      "0.2\t240\t80\tAUC 0.588975871948 at 21:51:07\n",
      "0.2\t340\t20\tAUC 0.58806239631 at 21:53:31\n",
      "0.2\t340\t40\tAUC 0.589345595929 at 21:55:46\n",
      "0.2\t340\t80\tAUC 0.588975871948 at 21:57:42\n",
      "Wall time: 1h 2min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "leaves = [20,40,80]\n",
    "n_estimators = [40,140,240,340]\n",
    "learning_rates = [.1,.2]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for n_tree in n_estimators:\n",
    "        for leaf in leaves:\n",
    "            gbm = lightgbm.LGBMClassifier(objective='binary:logistic',\n",
    "                            num_leaves=leaf,\n",
    "                            learning_rate=learning_rate,\n",
    "                            n_estimators=n_tree,\n",
    "                            feature_fraction=.8)\n",
    "\n",
    "            gbm.fit(X_train, y_train,\n",
    "                eval_set=[(X_test, y_test)],\n",
    "                eval_metric='auc',\n",
    "                early_stopping_rounds=2, verbose=False)\n",
    "\n",
    "            print '{}\\t{}\\t{}\\tAUC {} at {}'.format(learning_rate, n_tree, leaf, roc_auc_score(y_test, gbm.predict(X_test, num_iteration=gbm.best_iteration_)), datetime.datetime.now().strftime('%H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#re-fit model with best parameters\n",
    "\n",
    "gbm = lightgbm.LGBMClassifier(objective='binary:logistic',\n",
    "                            num_leaves=40,\n",
    "                            learning_rate=.2,\n",
    "                            n_estimators=140,\n",
    "                            feature_fraction=.8)\n",
    "\n",
    "gbm.fit(X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_metric='auc',\n",
    "        early_stopping_rounds=2, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.589743339731\n"
     ]
    }
   ],
   "source": [
    "pred = gbm.predict(X_test, num_iteration=gbm.best_iteration_)\n",
    "\n",
    "print roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1513334,   18063],\n",
       "       [ 134375,   31783]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(gbm, open('lgb.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "log = pickle.load(open('log.pickle', 'rb'))\n",
    "mlp = pickle.load(open('mlp.pickle', 'rb'))\n",
    "xgb = pickle.load(open('xgb.pickle', 'rb'))\n",
    "lgb = pickle.load(open('lgb.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_pred = log.predict(X_test)\n",
    "mlp_pred = mlp.predict(X_test)\n",
    "xgb_pred = xgb.predict(X_test)\n",
    "lgb_pred = lgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.010549844357798795, 1.0], [0.0, 0.011759197647638072, 1.0], [0.0, 0.011478408276887052, 1.0], [0.0, 0.011795112567152736, 1.0]]\n",
      "[[0.0, 0.1614487415592388, 1.0], [0.0, 0.17122858965562898, 1.0], [0.0, 0.18905499584732605, 1.0], [0.0, 0.19128179202927334, 1.0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "x_log, y_log, t_log = roc_curve(y_test, log_pred, pos_label=1)\n",
    "x_mlp, y_mlp, t_mlp = roc_curve(y_test, mlp_pred, pos_label=1)\n",
    "x_xgb, y_xgb, t_xgb = roc_curve(y_test, xgb_pred, pos_label=1)\n",
    "x_lgb, y_lgb, t_lgb = roc_curve(y_test, lgb_pred, pos_label=1)\n",
    "\n",
    "print [list(x_log), list(x_mlp), list(x_xgb), list(x_lgb)]\n",
    "print [list(y_log), list(y_mlp), list(y_xgb), list(y_lgb)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wOwwFp09Dp4RgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3a21fac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "names  = ['Logistic Regression', 'MLP', 'XGBoost', 'LightGBM']\n",
    "\n",
    "plt.plot(x_log, y_log)\n",
    "plt.plot(x_mlp, y_mlp)\n",
    "plt.plot(x_xgb, y_xgb)\n",
    "plt.plot(x_lgb, y_lgb)\n",
    "\n",
    "plt.legend(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>target</th>\n",
       "      <th>avg_order_size</th>\n",
       "      <th>prev_ord_size</th>\n",
       "      <th>avg_days_between_orders</th>\n",
       "      <th>num_orders_placed</th>\n",
       "      <th>reordered_usr_avg</th>\n",
       "      <th>overall_avg_prod_disp</th>\n",
       "      <th>...</th>\n",
       "      <th>usr_avg_aisle_disp</th>\n",
       "      <th>usr_avg_dept_disp</th>\n",
       "      <th>usr_avg_prod_disp</th>\n",
       "      <th>prod_due_overall_perc</th>\n",
       "      <th>prod_due_user_perc</th>\n",
       "      <th>aisle_due_overall_perc</th>\n",
       "      <th>aisle_due_user_perc</th>\n",
       "      <th>dept_due_overall_perc</th>\n",
       "      <th>dept_due_user_perc</th>\n",
       "      <th>reorder_custom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>3</td>\n",
       "      <td>2774568</td>\n",
       "      <td>248</td>\n",
       "      <td>2</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.09</td>\n",
       "      <td>13</td>\n",
       "      <td>0.625</td>\n",
       "      <td>109.905904</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>20.57</td>\n",
       "      <td>135.00</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>3</td>\n",
       "      <td>2774568</td>\n",
       "      <td>1005</td>\n",
       "      <td>2</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.09</td>\n",
       "      <td>13</td>\n",
       "      <td>0.625</td>\n",
       "      <td>108.401544</td>\n",
       "      <td>...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>45.00</td>\n",
       "      <td>37.00</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>3</td>\n",
       "      <td>2774568</td>\n",
       "      <td>1819</td>\n",
       "      <td>2</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.09</td>\n",
       "      <td>13</td>\n",
       "      <td>0.625</td>\n",
       "      <td>87.987620</td>\n",
       "      <td>...</td>\n",
       "      <td>23.5</td>\n",
       "      <td>23.50</td>\n",
       "      <td>31.33</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>3</td>\n",
       "      <td>2774568</td>\n",
       "      <td>7503</td>\n",
       "      <td>2</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.09</td>\n",
       "      <td>13</td>\n",
       "      <td>0.625</td>\n",
       "      <td>96.378691</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>20.57</td>\n",
       "      <td>114.00</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>3</td>\n",
       "      <td>2774568</td>\n",
       "      <td>8021</td>\n",
       "      <td>2</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.09</td>\n",
       "      <td>13</td>\n",
       "      <td>0.625</td>\n",
       "      <td>83.698469</td>\n",
       "      <td>...</td>\n",
       "      <td>135.0</td>\n",
       "      <td>135.00</td>\n",
       "      <td>135.00</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id  order_id  product_id  target  avg_order_size  prev_ord_size  \\\n",
       "120        3   2774568         248       2            7.33            6.0   \n",
       "121        3   2774568        1005       2            7.33            6.0   \n",
       "122        3   2774568        1819       2            7.33            6.0   \n",
       "123        3   2774568        7503       2            7.33            6.0   \n",
       "124        3   2774568        8021       2            7.33            6.0   \n",
       "\n",
       "     avg_days_between_orders  num_orders_placed  reordered_usr_avg  \\\n",
       "120                    12.09                 13              0.625   \n",
       "121                    12.09                 13              0.625   \n",
       "122                    12.09                 13              0.625   \n",
       "123                    12.09                 13              0.625   \n",
       "124                    12.09                 13              0.625   \n",
       "\n",
       "     overall_avg_prod_disp       ...        usr_avg_aisle_disp  \\\n",
       "120             109.905904       ...                      45.0   \n",
       "121             108.401544       ...                      37.0   \n",
       "122              87.987620       ...                      23.5   \n",
       "123              96.378691       ...                      45.0   \n",
       "124              83.698469       ...                     135.0   \n",
       "\n",
       "     usr_avg_dept_disp  usr_avg_prod_disp  prod_due_overall_perc  \\\n",
       "120              20.57             135.00                  0.231   \n",
       "121              45.00              37.00                  0.077   \n",
       "122              23.50              31.33                  0.308   \n",
       "123              20.57             114.00                  0.231   \n",
       "124             135.00             135.00                  0.077   \n",
       "\n",
       "     prod_due_user_perc  aisle_due_overall_perc  aisle_due_user_perc  \\\n",
       "120               0.538                   0.625                0.231   \n",
       "121               0.231                   0.625                0.077   \n",
       "122               0.308                   0.625                0.308   \n",
       "123               0.538                   0.625                0.231   \n",
       "124               0.077                   0.625                0.077   \n",
       "\n",
       "     dept_due_overall_perc  dept_due_user_perc  reorder_custom  \n",
       "120                  0.538               0.625               0  \n",
       "121                  0.231               0.625               0  \n",
       "122                  0.308               0.625               0  \n",
       "123                  0.538               0.625               0  \n",
       "124                  0.077               0.625               0  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('x_test.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.as_matrix(columns=df.columns[4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "log = pickle.load(open('log.pickle', 'rb'))\n",
    "mlp = pickle.load(open('mlp.pickle', 'rb'))\n",
    "xgb = pickle.load(open('xgb.pickle', 'rb'))\n",
    "lgb = pickle.load(open('lgb.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>avg_order_size</th>\n",
       "      <th>prev_ord_size</th>\n",
       "      <th>log_pred</th>\n",
       "      <th>log_prob</th>\n",
       "      <th>mlp_pred</th>\n",
       "      <th>mlp_prob</th>\n",
       "      <th>xgb_pred</th>\n",
       "      <th>xgb_prob</th>\n",
       "      <th>lgb_pred</th>\n",
       "      <th>lgb_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2774568</td>\n",
       "      <td>248</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.972031</td>\n",
       "      <td>0</td>\n",
       "      <td>0.984377</td>\n",
       "      <td>0</td>\n",
       "      <td>0.970881</td>\n",
       "      <td>0</td>\n",
       "      <td>0.979271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2774568</td>\n",
       "      <td>1005</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.942469</td>\n",
       "      <td>0</td>\n",
       "      <td>0.948694</td>\n",
       "      <td>0</td>\n",
       "      <td>0.953822</td>\n",
       "      <td>0</td>\n",
       "      <td>0.950803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2774568</td>\n",
       "      <td>1819</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.867264</td>\n",
       "      <td>0</td>\n",
       "      <td>0.903163</td>\n",
       "      <td>0</td>\n",
       "      <td>0.878013</td>\n",
       "      <td>0</td>\n",
       "      <td>0.902975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2774568</td>\n",
       "      <td>7503</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.968177</td>\n",
       "      <td>0</td>\n",
       "      <td>0.976053</td>\n",
       "      <td>0</td>\n",
       "      <td>0.971565</td>\n",
       "      <td>0</td>\n",
       "      <td>0.966158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2774568</td>\n",
       "      <td>8021</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.954667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.980414</td>\n",
       "      <td>0</td>\n",
       "      <td>0.965675</td>\n",
       "      <td>0</td>\n",
       "      <td>0.974175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     order_id  product_id  avg_order_size  prev_ord_size  log_pred  log_prob  \\\n",
       "120   2774568         248            7.33            6.0         0  0.972031   \n",
       "121   2774568        1005            7.33            6.0         0  0.942469   \n",
       "122   2774568        1819            7.33            6.0         0  0.867264   \n",
       "123   2774568        7503            7.33            6.0         0  0.968177   \n",
       "124   2774568        8021            7.33            6.0         0  0.954667   \n",
       "\n",
       "     mlp_pred  mlp_prob  xgb_pred  xgb_prob  lgb_pred  lgb_prob  \n",
       "120         0  0.984377         0  0.970881         0  0.979271  \n",
       "121         0  0.948694         0  0.953822         0  0.950803  \n",
       "122         0  0.903163         0  0.878013         0  0.902975  \n",
       "123         0  0.976053         0  0.971565         0  0.966158  \n",
       "124         0  0.980414         0  0.965675         0  0.974175  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['order_id', 'product_id', 'avg_order_size', 'prev_ord_size']]\n",
    "\n",
    "df['log_pred'] = log.predict(X)\n",
    "df['log_prob'] = log.predict_proba(X)[:,0]\n",
    "\n",
    "df['mlp_pred'] = mlp.predict(X)\n",
    "df['mlp_prob'] = mlp.predict_proba(X)[:,0]\n",
    "\n",
    "df['xgb_pred'] = xgb.predict(X)\n",
    "df['xgb_prob'] = xgb.predict_proba(X)[:,0]\n",
    "\n",
    "df['lgb_pred'] = lgb.predict(X)\n",
    "df['lgb_prob'] = lgb.predict_proba(X)[:,0]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4710200\n",
      "1     123092\n",
      "Name: log_pred, dtype: int64\n",
      "0    4698373\n",
      "1     134919\n",
      "Name: mlp_pred, dtype: int64\n",
      "0    4692413\n",
      "1     140879\n",
      "Name: xgb_pred, dtype: int64\n",
      "0    4690551\n",
      "1     142741\n",
      "Name: lgb_pred, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df['log_pred'].value_counts()\n",
    "print df['mlp_pred'].value_counts()\n",
    "print df['xgb_pred'].value_counts()\n",
    "print df['lgb_pred'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that the positive class in the train set was about 10% of the rows, and all models are dramatically under-predicting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.900562425855 0.020022033575 0.999231759337\n"
     ]
    }
   ],
   "source": [
    "print df['lgb_prob'].mean(), df['lgb_prob'].min(), df['lgb_prob'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The mean probility is 90%!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>avg_order_size</th>\n",
       "      <th>prev_ord_size</th>\n",
       "      <th>log_pred</th>\n",
       "      <th>log_prob</th>\n",
       "      <th>mlp_pred</th>\n",
       "      <th>mlp_prob</th>\n",
       "      <th>xgb_pred</th>\n",
       "      <th>xgb_prob</th>\n",
       "      <th>lgb_pred</th>\n",
       "      <th>lgb_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2774568</td>\n",
       "      <td>38596</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.973590</td>\n",
       "      <td>0</td>\n",
       "      <td>0.984498</td>\n",
       "      <td>0</td>\n",
       "      <td>0.984444</td>\n",
       "      <td>0</td>\n",
       "      <td>0.990943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>2774568</td>\n",
       "      <td>15143</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.969365</td>\n",
       "      <td>0</td>\n",
       "      <td>0.975996</td>\n",
       "      <td>0</td>\n",
       "      <td>0.987458</td>\n",
       "      <td>0</td>\n",
       "      <td>0.987098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2774568</td>\n",
       "      <td>248</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.972031</td>\n",
       "      <td>0</td>\n",
       "      <td>0.984377</td>\n",
       "      <td>0</td>\n",
       "      <td>0.970881</td>\n",
       "      <td>0</td>\n",
       "      <td>0.979271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>2774568</td>\n",
       "      <td>8021</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.954667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.980414</td>\n",
       "      <td>0</td>\n",
       "      <td>0.965675</td>\n",
       "      <td>0</td>\n",
       "      <td>0.974175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2774568</td>\n",
       "      <td>40604</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.959093</td>\n",
       "      <td>0</td>\n",
       "      <td>0.982754</td>\n",
       "      <td>0</td>\n",
       "      <td>0.971955</td>\n",
       "      <td>0</td>\n",
       "      <td>0.971412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>2774568</td>\n",
       "      <td>39922</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.960637</td>\n",
       "      <td>0</td>\n",
       "      <td>0.969783</td>\n",
       "      <td>0</td>\n",
       "      <td>0.971800</td>\n",
       "      <td>0</td>\n",
       "      <td>0.968928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>2774568</td>\n",
       "      <td>7503</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.968177</td>\n",
       "      <td>0</td>\n",
       "      <td>0.976053</td>\n",
       "      <td>0</td>\n",
       "      <td>0.971565</td>\n",
       "      <td>0</td>\n",
       "      <td>0.966158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>2774568</td>\n",
       "      <td>12845</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.958715</td>\n",
       "      <td>0</td>\n",
       "      <td>0.956073</td>\n",
       "      <td>0</td>\n",
       "      <td>0.968334</td>\n",
       "      <td>0</td>\n",
       "      <td>0.964373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>2774568</td>\n",
       "      <td>49683</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.936041</td>\n",
       "      <td>0</td>\n",
       "      <td>0.979806</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955339</td>\n",
       "      <td>0</td>\n",
       "      <td>0.962459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2774568</td>\n",
       "      <td>18370</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.944245</td>\n",
       "      <td>0</td>\n",
       "      <td>0.951660</td>\n",
       "      <td>0</td>\n",
       "      <td>0.959527</td>\n",
       "      <td>0</td>\n",
       "      <td>0.957008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2774568</td>\n",
       "      <td>1005</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.942469</td>\n",
       "      <td>0</td>\n",
       "      <td>0.948694</td>\n",
       "      <td>0</td>\n",
       "      <td>0.953822</td>\n",
       "      <td>0</td>\n",
       "      <td>0.950803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2774568</td>\n",
       "      <td>42557</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.939616</td>\n",
       "      <td>0</td>\n",
       "      <td>0.947456</td>\n",
       "      <td>0</td>\n",
       "      <td>0.946050</td>\n",
       "      <td>0</td>\n",
       "      <td>0.948325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>2774568</td>\n",
       "      <td>21137</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.933430</td>\n",
       "      <td>0</td>\n",
       "      <td>0.949220</td>\n",
       "      <td>0</td>\n",
       "      <td>0.937255</td>\n",
       "      <td>0</td>\n",
       "      <td>0.945567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2774568</td>\n",
       "      <td>16965</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.924580</td>\n",
       "      <td>0</td>\n",
       "      <td>0.929186</td>\n",
       "      <td>0</td>\n",
       "      <td>0.945507</td>\n",
       "      <td>0</td>\n",
       "      <td>0.940436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2774568</td>\n",
       "      <td>42265</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.916868</td>\n",
       "      <td>0</td>\n",
       "      <td>0.941857</td>\n",
       "      <td>0</td>\n",
       "      <td>0.937900</td>\n",
       "      <td>0</td>\n",
       "      <td>0.938234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2774568</td>\n",
       "      <td>28373</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.892748</td>\n",
       "      <td>0</td>\n",
       "      <td>0.932161</td>\n",
       "      <td>0</td>\n",
       "      <td>0.925345</td>\n",
       "      <td>0</td>\n",
       "      <td>0.927784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2774568</td>\n",
       "      <td>24010</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.907958</td>\n",
       "      <td>0</td>\n",
       "      <td>0.911924</td>\n",
       "      <td>0</td>\n",
       "      <td>0.928436</td>\n",
       "      <td>0</td>\n",
       "      <td>0.925163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>2774568</td>\n",
       "      <td>44683</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.901815</td>\n",
       "      <td>0</td>\n",
       "      <td>0.912604</td>\n",
       "      <td>0</td>\n",
       "      <td>0.926193</td>\n",
       "      <td>0</td>\n",
       "      <td>0.922615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>2774568</td>\n",
       "      <td>14992</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.902626</td>\n",
       "      <td>0</td>\n",
       "      <td>0.917307</td>\n",
       "      <td>0</td>\n",
       "      <td>0.928165</td>\n",
       "      <td>0</td>\n",
       "      <td>0.921352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2774568</td>\n",
       "      <td>1819</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.867264</td>\n",
       "      <td>0</td>\n",
       "      <td>0.903163</td>\n",
       "      <td>0</td>\n",
       "      <td>0.878013</td>\n",
       "      <td>0</td>\n",
       "      <td>0.902975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>2774568</td>\n",
       "      <td>48523</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.889324</td>\n",
       "      <td>0</td>\n",
       "      <td>0.892289</td>\n",
       "      <td>0</td>\n",
       "      <td>0.908783</td>\n",
       "      <td>0</td>\n",
       "      <td>0.893435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2774568</td>\n",
       "      <td>9387</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.777970</td>\n",
       "      <td>0</td>\n",
       "      <td>0.822598</td>\n",
       "      <td>0</td>\n",
       "      <td>0.882747</td>\n",
       "      <td>0</td>\n",
       "      <td>0.880213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>2774568</td>\n",
       "      <td>23650</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.802613</td>\n",
       "      <td>0</td>\n",
       "      <td>0.773737</td>\n",
       "      <td>0</td>\n",
       "      <td>0.829062</td>\n",
       "      <td>0</td>\n",
       "      <td>0.837859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2774568</td>\n",
       "      <td>16797</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.787521</td>\n",
       "      <td>0</td>\n",
       "      <td>0.747540</td>\n",
       "      <td>0</td>\n",
       "      <td>0.838107</td>\n",
       "      <td>0</td>\n",
       "      <td>0.836140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>2774568</td>\n",
       "      <td>22035</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.818462</td>\n",
       "      <td>0</td>\n",
       "      <td>0.813032</td>\n",
       "      <td>0</td>\n",
       "      <td>0.838208</td>\n",
       "      <td>0</td>\n",
       "      <td>0.828512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2774568</td>\n",
       "      <td>32402</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.879963</td>\n",
       "      <td>0</td>\n",
       "      <td>0.868190</td>\n",
       "      <td>0</td>\n",
       "      <td>0.862395</td>\n",
       "      <td>0</td>\n",
       "      <td>0.825129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2774568</td>\n",
       "      <td>24810</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.787962</td>\n",
       "      <td>0</td>\n",
       "      <td>0.778464</td>\n",
       "      <td>0</td>\n",
       "      <td>0.789268</td>\n",
       "      <td>0</td>\n",
       "      <td>0.762548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>2774568</td>\n",
       "      <td>18599</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.710856</td>\n",
       "      <td>0</td>\n",
       "      <td>0.679559</td>\n",
       "      <td>0</td>\n",
       "      <td>0.702343</td>\n",
       "      <td>0</td>\n",
       "      <td>0.692872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2774568</td>\n",
       "      <td>17668</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.729937</td>\n",
       "      <td>0</td>\n",
       "      <td>0.656483</td>\n",
       "      <td>0</td>\n",
       "      <td>0.668353</td>\n",
       "      <td>0</td>\n",
       "      <td>0.664003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2774568</td>\n",
       "      <td>43961</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.762352</td>\n",
       "      <td>0</td>\n",
       "      <td>0.609207</td>\n",
       "      <td>0</td>\n",
       "      <td>0.629194</td>\n",
       "      <td>0</td>\n",
       "      <td>0.609769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2774568</td>\n",
       "      <td>21903</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.320603</td>\n",
       "      <td>1</td>\n",
       "      <td>0.486110</td>\n",
       "      <td>1</td>\n",
       "      <td>0.431377</td>\n",
       "      <td>1</td>\n",
       "      <td>0.354365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>2774568</td>\n",
       "      <td>47766</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.254825</td>\n",
       "      <td>1</td>\n",
       "      <td>0.459466</td>\n",
       "      <td>1</td>\n",
       "      <td>0.252734</td>\n",
       "      <td>1</td>\n",
       "      <td>0.269483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>2774568</td>\n",
       "      <td>39190</td>\n",
       "      <td>7.33</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.188681</td>\n",
       "      <td>1</td>\n",
       "      <td>0.459158</td>\n",
       "      <td>1</td>\n",
       "      <td>0.169380</td>\n",
       "      <td>1</td>\n",
       "      <td>0.172244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     order_id  product_id  avg_order_size  prev_ord_size  log_pred  log_prob  \\\n",
       "142   2774568       38596            7.33            6.0         0  0.973590   \n",
       "128   2774568       15143            7.33            6.0         0  0.969365   \n",
       "120   2774568         248            7.33            6.0         0  0.972031   \n",
       "124   2774568        8021            7.33            6.0         0  0.954667   \n",
       "145   2774568       40604            7.33            6.0         0  0.959093   \n",
       "144   2774568       39922            7.33            6.0         0  0.960637   \n",
       "123   2774568        7503            7.33            6.0         0  0.968177   \n",
       "126   2774568       12845            7.33            6.0         0  0.958715   \n",
       "152   2774568       49683            7.33            6.0         0  0.936041   \n",
       "132   2774568       18370            7.33            6.0         0  0.944245   \n",
       "121   2774568        1005            7.33            6.0         0  0.942469   \n",
       "147   2774568       42557            7.33            6.0         0  0.939616   \n",
       "134   2774568       21137            7.33            6.0         0  0.933430   \n",
       "130   2774568       16965            7.33            6.0         0  0.924580   \n",
       "146   2774568       42265            7.33            6.0         0  0.916868   \n",
       "140   2774568       28373            7.33            6.0         0  0.892748   \n",
       "138   2774568       24010            7.33            6.0         0  0.907958   \n",
       "149   2774568       44683            7.33            6.0         0  0.901815   \n",
       "127   2774568       14992            7.33            6.0         0  0.902626   \n",
       "122   2774568        1819            7.33            6.0         0  0.867264   \n",
       "151   2774568       48523            7.33            6.0         0  0.889324   \n",
       "125   2774568        9387            7.33            6.0         0  0.777970   \n",
       "137   2774568       23650            7.33            6.0         0  0.802613   \n",
       "129   2774568       16797            7.33            6.0         0  0.787521   \n",
       "136   2774568       22035            7.33            6.0         0  0.818462   \n",
       "141   2774568       32402            7.33            6.0         0  0.879963   \n",
       "139   2774568       24810            7.33            6.0         0  0.787962   \n",
       "133   2774568       18599            7.33            6.0         0  0.710856   \n",
       "131   2774568       17668            7.33            6.0         0  0.729937   \n",
       "148   2774568       43961            7.33            6.0         0  0.762352   \n",
       "135   2774568       21903            7.33            6.0         1  0.320603   \n",
       "150   2774568       47766            7.33            6.0         1  0.254825   \n",
       "143   2774568       39190            7.33            6.0         1  0.188681   \n",
       "\n",
       "     mlp_pred  mlp_prob  xgb_pred  xgb_prob  lgb_pred  lgb_prob  \n",
       "142         0  0.984498         0  0.984444         0  0.990943  \n",
       "128         0  0.975996         0  0.987458         0  0.987098  \n",
       "120         0  0.984377         0  0.970881         0  0.979271  \n",
       "124         0  0.980414         0  0.965675         0  0.974175  \n",
       "145         0  0.982754         0  0.971955         0  0.971412  \n",
       "144         0  0.969783         0  0.971800         0  0.968928  \n",
       "123         0  0.976053         0  0.971565         0  0.966158  \n",
       "126         0  0.956073         0  0.968334         0  0.964373  \n",
       "152         0  0.979806         0  0.955339         0  0.962459  \n",
       "132         0  0.951660         0  0.959527         0  0.957008  \n",
       "121         0  0.948694         0  0.953822         0  0.950803  \n",
       "147         0  0.947456         0  0.946050         0  0.948325  \n",
       "134         0  0.949220         0  0.937255         0  0.945567  \n",
       "130         0  0.929186         0  0.945507         0  0.940436  \n",
       "146         0  0.941857         0  0.937900         0  0.938234  \n",
       "140         0  0.932161         0  0.925345         0  0.927784  \n",
       "138         0  0.911924         0  0.928436         0  0.925163  \n",
       "149         0  0.912604         0  0.926193         0  0.922615  \n",
       "127         0  0.917307         0  0.928165         0  0.921352  \n",
       "122         0  0.903163         0  0.878013         0  0.902975  \n",
       "151         0  0.892289         0  0.908783         0  0.893435  \n",
       "125         0  0.822598         0  0.882747         0  0.880213  \n",
       "137         0  0.773737         0  0.829062         0  0.837859  \n",
       "129         0  0.747540         0  0.838107         0  0.836140  \n",
       "136         0  0.813032         0  0.838208         0  0.828512  \n",
       "141         0  0.868190         0  0.862395         0  0.825129  \n",
       "139         0  0.778464         0  0.789268         0  0.762548  \n",
       "133         0  0.679559         0  0.702343         0  0.692872  \n",
       "131         0  0.656483         0  0.668353         0  0.664003  \n",
       "148         0  0.609207         0  0.629194         0  0.609769  \n",
       "135         1  0.486110         1  0.431377         1  0.354365  \n",
       "150         1  0.459466         1  0.252734         1  0.269483  \n",
       "143         1  0.459158         1  0.169380         1  0.172244  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['order_id']==2774568].sort_values('lgb_prob', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average probability is too high!\n",
    "There's no separation to differentiate user-product pairs.\n",
    "See Final Analysis notebook for a different approach, and check out my [blog post](https://p-mckenzie.github.io/2017/12/12/instacart-part-2/ \"Instacart Part 2 - Modeling\") on the subject."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
